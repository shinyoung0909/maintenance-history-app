# íŒŒì¼ëª…: app.py
import os
import pandas as pd
import re
from collections import defaultdict, Counter
import streamlit as st
import plotly.express as px
import time

from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

st.set_page_config(page_title="ë°˜ë„ì²´ ì •ë¹„ë…¸íŠ¸ ê²€ìƒ‰&ì¶”ì²œ", layout="wide")
st.title("ğŸ”§ ë°˜ë„ì²´ ì •ë¹„ë…¸íŠ¸ ê²€ìƒ‰ & ì„±ê³µë¥  ì¶”ì²œ")

# OpenAI API Key
api_key = st.text_input("ğŸ”‘ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
if not api_key:
    st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    st.stop()
os.environ["OPENAI_API_KEY"] = api_key

# ì—‘ì…€ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"])
if uploaded_file is None:
    st.info("ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    st.stop()

df = pd.read_excel(uploaded_file)
df = df.dropna(subset=['ì •ë¹„ë…¸íŠ¸'])
st.success(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: ì´ {len(df)} í–‰")

# -----------------------------
# í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„±ê³µë¥  ê³„ì‚°
# -----------------------------
lines = []
for note in df["ì •ë¹„ë…¸íŠ¸"].astype(str):
    for line in note.split('\n'):
        text = re.sub(r'^\d{2}ì›”\d{2}ì¼ \d{2}:\d{2} ', '', line).strip()
        if text and not text.startswith("LOT m951990 ì²˜ë¦¬ ì¤‘ abnormal í˜„ìƒ ê°ì§€"):
            lines.append(text)

cause_pattern = re.compile(r'LOT ì§„í–‰ ì¤‘ (.+) ë°œìƒ')
first_action_pattern = re.compile(r'1ì°¨ ì¡°ì¹˜: (.+) â†’ ì—¬ì „íˆ ì´ìƒ ë°œìƒ')
second_action_pattern = re.compile(r'ì •ë¹„ ì‹œì‘\. (.+) ì§„í–‰')
third_action_pattern = re.compile(r'ì¶”ê°€ ì¡°ì¹˜: (.+)')

cause_aliases = {
    "wafer not ë°œìƒ": "wafer not",
    "wafer not ê°ì§€ë¨": "wafer not",
    "wafer not ë°œìƒ í™•ì¸": "wafer not",
}

def normalize_cause(cause):
    for alias, norm in cause_aliases.items():
        if alias in cause:
            return norm
    return cause

cause_action_counts = defaultdict(lambda: defaultdict(Counter))
cause = None

for line in lines:
    if (m := cause_pattern.search(line)):
        cause = normalize_cause(m.group(1).strip())
        continue
    if cause is None:
        continue

    if (m1 := first_action_pattern.search(line)):
        cause_action_counts[cause][m1.group(1).strip()]['first'] += 1
    elif (m2 := second_action_pattern.search(line)):
        cause_action_counts[cause][m2.group(1).strip()]['second'] += 1
    elif (m3 := third_action_pattern.search(line)):
        cause_action_counts[cause][m3.group(1).strip()]['third'] += 1

rows = []
for cause, actions in cause_action_counts.items():
    for action, counts in actions.items():
        f, s, t = counts.get('first', 0), counts.get('second', 0), counts.get('third', 0)
        total, success = f + s + t, s + t
        rate = round(success / total * 100, 2) if total > 0 else 0
        rows.append({"ëŒ€í‘œì›ì¸": cause, "ì¡°ì¹˜": action, "ì´íšŸìˆ˜": total, "ì‹¤íŒ¨íšŸìˆ˜": f, "ì„±ê³µíšŸìˆ˜": success, "ì„±ê³µë¥ (%)": rate})

df_success = pd.DataFrame(rows)

# -----------------------------
# ë²¡í„° DB êµ¬ì„± (Chroma ì‚¬ìš©)
# -----------------------------
documents = [Document(page_content=str(row['ì •ë¹„ë…¸íŠ¸']), metadata={'row': idx}) for idx, row in df.iterrows()]
split_docs = CharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(documents)
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
vectordb = Chroma.from_documents(documents=split_docs, embedding=embedding_model)
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb.as_retriever(search_kwargs={'k': 20}), return_source_documents=True)

# -----------------------------
# UI
# -----------------------------
tab1, tab2 = st.tabs(["ğŸ” ì •ë¹„ ê²€ìƒ‰ & ì¶”ì²œ", "ğŸ“ˆ í†µê³„ ìë£Œ"])

with tab1:
    example_keywords = ["wafer not", "plasma ignition failure", "pumpdown ì‹œê°„ ì§€ì—°", "slot valve ë™ì‘ ë¶ˆëŸ‰", "RF auto match ë¶ˆëŸ‰"]
    st.markdown(f"<p style='font-size:18px;'>ğŸ’¡ ì˜ˆì‹œ í‚¤ì›Œë“œ: {' | '.join(example_keywords)}</p>", unsafe_allow_html=True)
    st.markdown("<h3>ê²€ìƒ‰í•  ì •ë¹„ ì´ìŠˆë¥¼ ì…ë ¥í•˜ì„¸ìš”</h3>", unsafe_allow_html=True)
    query = st.text_input("", placeholder="ì˜ˆ: plasma ignition failure", key="query")

    if query.strip():
        bar = st.progress(0)
        for p in range(0, 101, 10): time.sleep(0.05); bar.progress(p)
        bar.empty()
        st.success(f"'{query}' ê²€ìƒ‰ ì™„ë£Œ")

        output = qa_chain({"query": query})
        docs = output["source_documents"]

        recommended = []
        for doc in docs:
            note = doc.page_content
            row_idx = doc.metadata['row']
            for _, row in df_success.iterrows():
                if row["ì¡°ì¹˜"] in note:
                    recommended.append({
                        "ì¡°ì¹˜": row["ì¡°ì¹˜"],
                        "ì„±ê³µë¥ ": row["ì„±ê³µë¥ (%)"],
                        "ì¥ë¹„ID": df.loc[row_idx, 'ì¥ë¹„ID'] if 'ì¥ë¹„ID' in df.columns else 'N/A',
                        "ëª¨ë¸": df.loc[row_idx, 'ëª¨ë¸'] if 'ëª¨ë¸' in df.columns else 'N/A',
                        "ì •ë¹„ì¢…ë¥˜": df.loc[row_idx, 'ì •ë¹„ì¢…ë¥˜'] if 'ì •ë¹„ì¢…ë¥˜' in df.columns else 'N/A',
                        "ì •ë¹„ì": df.loc[row_idx, 'ì •ë¹„ì'] if 'ì •ë¹„ì' in df.columns else 'N/A',
                        "ì •ë¹„ë…¸íŠ¸": note
                    })

        if not recommended:
            st.warning("â— ìœ ì‚¬ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            dedup = {}
            for r in recommended:
                act = r["ì¡°ì¹˜"]
                if act not in dedup or dedup[act]["ì„±ê³µë¥ "] < r["ì„±ê³µë¥ "]:
                    dedup[act] = r
            top3 = sorted(dedup.values(), key=lambda x: x["ì„±ê³µë¥ "], reverse=True)[:3]

            st.subheader("âœ… ì„±ê³µë¥  ìƒìœ„ 3ê°œ ì¡°ì¹˜")
            for idx, r in enumerate(top3, 1):
                st.markdown(f"**{idx}) {r['ì¡°ì¹˜']}** - ì„±ê³µë¥  {r['ì„±ê³µë¥ ']}%")

            st.markdown("### ğŸ§¾ ëŒ€í‘œ ì‚¬ë¡€")
            note_html = top3[0]["ì •ë¹„ë…¸íŠ¸"].replace("\n", "<br>")
            st.markdown(f"""
<div style="border:1px solid #ccc; padding:15px; background:#f9f9f9;">
<b>ì¡°ì¹˜ëª…:</b> {top3[0]['ì¡°ì¹˜']}<br>
<b>ì¥ë¹„:</b> {top3[0]['ì¥ë¹„ID']} / {top3[0]['ëª¨ë¸']}<br>
<b>ì •ë¹„ì¢…ë¥˜:</b> {top3[0]['ì •ë¹„ì¢…ë¥˜']}<br>
<b>ì •ë¹„ì:</b> {top3[0]['ì •ë¹„ì']}<br><br>
<b>ì •ë¹„ë…¸íŠ¸:</b><br>{note_html}
</div>
""", unsafe_allow_html=True)

with tab2:
    st.subheader("ğŸ“Š í†µê³„ ë¶„ì„")

    top5_equip = df['ëª¨ë¸'].value_counts().head(5)
    fig1 = px.bar(x=top5_equip.index, y=top5_equip.values, text=top5_equip.values, color=top5_equip.values)
    st.plotly_chart(fig1, use_container_width=True)

    top5_cause = df_success.groupby('ëŒ€í‘œì›ì¸')['ì´íšŸìˆ˜'].sum().nlargest(5)
    fig2 = px.bar(x=top5_cause.values, y=top5_cause.index, orientation='h', text=top5_cause.values)
    st.plotly_chart(fig2, use_container_width=True)

    df_success['row'] = df_success.index
    df_joined = df.merge(df_success, left_index=True, right_on='row', how='left')
    eng_stats = df_joined.groupby("ì •ë¹„ì")["ì„±ê³µë¥ (%)"].mean().dropna().round(1).sort_values(ascending=False).head(5)
    fig3 = px.bar(x=eng_stats.index, y=eng_stats.values, text=eng_stats.values, color=eng_stats.values)
    st.plotly_chart(fig3, use_container_width=True)
